import torch.nn as nn
import torch

class ResidualBlock(nn.Module):
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 stride: int = 1,
                 downsample: nn.Module = None
                 ):
        super().__init__()

        self.layers = nn.Sequential(
            nn.BatchNorm2d(in_channels),
            nn.ReLU(),
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)
        )


        self.downsample = downsample

    def forward(self, x: torch.Tensor):
        residual = x
        x = self.layers(x)

        if self.downsample is not None:
            residual = self.downsample(residual)

        x += residual
        return x
    

class Bottleneck(nn.Module):
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 stride: int = 1,
                 downsample: nn.Module = None
                 ):
        super().__init__()

        self.layers = nn.Sequential(
            nn.BatchNorm2d(in_channels),
            nn.ReLU(),
            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, stride=stride, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, 4 * out_channels, kernel_size=1, padding=0, stride=1, bias=False)
            )

        self.downsample = downsample


    def forward(self, x: torch.Tensor):
        residual = x
        x = self.layers(x)

        if self.downsample is not None:
            residual = self.downsample(residual)

        x += residual
        return x
    

class PreActResNetBlock(nn.Sequential):
    def __init__(self,
                 block:nn.Module,
                 in_channels: int,
                 out_channels: int,
                 num_blocks: int,
                 stride: int,
                 expansion: int
                 ):
        

        downsample = None
        if stride != 1 or in_channels != out_channels * expansion:
            downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels * expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * expansion)
            )

        layers = []
        layers.append(block(in_channels, out_channels, stride=stride, downsample=downsample))


        # after first block, in_channels is changed to out_channels * expansion
        for i in range(num_blocks-1):
            layers.append(block(out_channels * expansion, out_channels, stride=1, downsample=None))

        # as it is inherited from nn.Sequential, forward() method automatically defined
        super().__init__(*layers)
    

class PreActResNet(nn.Module):
    def __init__(self,
                 block_list: list[int],
                 num_classes:int,
                 bottleneck: bool,
                 is_data_small:bool):
        super().__init__()


        if bottleneck:
            expansion = 4
            block = Bottleneck
        else:
            expansion = 1
            block = ResidualBlock


        if is_data_small:
            self.conv1 = nn.Sequential(
                nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),
                nn.BatchNorm2d(16),
                nn.ReLU()
            )

            self.conv2 = PreActResNetBlock(block, 16, 16,
                                     block_list[0], stride=1, expansion=expansion) 
            self.conv3 = PreActResNetBlock(block, 16 * expansion, 32,
                                     block_list[1], stride=2, expansion=expansion)
            self.conv4 = PreActResNetBlock(block, 32 * expansion, 64,
                                     block_list[2], stride=2, expansion=expansion)
            self.conv5 = None
            self.classifier = nn.Sequential(
                nn.BatchNorm2d(64 * expansion),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten(),
                nn.Linear(64 * expansion, num_classes)
            )

        else:
            self.conv1 = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2)
            )

            self.conv2 = PreActResNetBlock(block, 64, 64,
                                     block_list[0], stride=1, expansion=expansion) 
            self.conv3 = PreActResNetBlock(block, 64 * expansion, 128,
                                     block_list[1], stride=2, expansion=expansion)
            self.conv4 = PreActResNetBlock(block, 128 * expansion, 256,
                                     block_list[2], stride=2, expansion=expansion)
            self.conv5 = PreActResNetBlock(block, 256 * expansion, 512,
                                     block_list[3], stride=2, expansion=expansion)
            self.classifier = nn.Sequential(
                nn.BatchNorm2d(512 * expansion),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten(),
                nn.Linear(512 * expansion, num_classes)
            )

        # nn.Module.apply() recursively applies the given function to every submodule
        # https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Conv2d):
            """
            He Normal Initialization
            - initialization is based on https://arxiv.org/abs/1502.01852
            - normal gaussian with sqrt(2. / out units) std with zero mean
            """
            #
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
            if module.bias is not None:
                nn.init.zeros_(module.bias)

        elif isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, 0, 0.01)
            nn.init.zeros_(module.bias)


    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)

        if self.conv5 is not None:
            x = self.conv5(x)

        out = self.classifier(x)
        return out
    

def get_preactresnet(model_name: str, num_classes: int, is_data_small: bool):

    model_config_dict = {
        'preactresnet110': ([18, 18, 18], False,is_data_small),
        'preactresnet164': ([18, 18, 18], True, is_data_small),
        'preactresnet200': ([3, 24, 36, 3], True, is_data_small), # for ImageNet
        'preactresnet1001': ([111, 111, 111], True, is_data_small)
    }

    if model_name not in model_config_dict.keys():
        raise ValueError(f"Given model name does not exit in PreActResNet model config. Got {model_name}")

    model_config = model_config_dict[model_name]
    return PreActResNet(model_config[0], num_classes, model_config[1], model_config[2])


if __name__ == '__main__':
    import torch

    def model_summary(model: nn.Module):
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p  in model.parameters() if p.requires_grad)

        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")


    model = get_preactresnet('preactresnet200', 10, False) # for comparison with # of params in table 6

    model_summary(model)

    with torch.no_grad():
        device = 'cpu'
        model.eval()

        data = torch.randn(2, 3, 224, 224).to(device)
        output = model(data)
        pred = output.argmax(dim=-1)

        print(f"Output shape: {output.shape}")
        print(f"Predictions: {pred}")

    ###################################
    # Complete Model Checking



        


    
    

        


    